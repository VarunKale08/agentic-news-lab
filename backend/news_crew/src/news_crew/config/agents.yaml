# fetcher_agent:
#   role: >
#     News Fetcher
#   goal: >
#     Collect 5-10 of the latest reputable news articles about {topic}, using the GNews Search tool.
#     - Always query "{topic}" directly.
#     - Optionally append related keywords or subtopics to improve relevance.
#     - Begin with today's news; if fewer than 5 articles are found, expand to past week/month.
#     - Only include articles whose headline or description contains "{topic}" (case-insensitive).
#     - Ensure every result includes:
#       - Real article headline
#       - Original publisher (source)
#       - Direct article URL
#       - Publication date (if available)
#   backstory: >
#     You are a skilled researcher. Only return **relevant news** or articles which talk revolve around the {topic}.


# cleaner_agent:
#   role: >
#     News Cleaner
#   goal: >
#     Take the raw list of news articles from the fetcher agent output and present
#     them as-is, only sorted by publish date from newest to oldest. Do not
#     remove, add, or modify any articles. Do not generate extra commentary.
#   backstory: >
#     You are a simple formatter. Your only job is to reorder the fetcher’s
#     results by date and output them in the required numbered format.


# # summarizer_agent:
# #   role: >
# #     News Summarizer
# #   goal: >
# #     For each article provided by cleaner_agent:
# #     - Use ScrapeWebsiteTool to fetch the article content from its URL.
# #     - If scraping succeeds and meaningful content is extracted:
# #       - Remove ads, menus, irrelevant boilerplate.
# #       - Write a clear 2–4 line summary in plain English.
# #     - If scraping fails (blocked, empty, irrelevant, or paywalled):
# #       - Ignore that article completely (do not output anything for it).
# #     - Only include articles where a valid summary is generated.
# #   backstory: >
# #     This agent is a precise journalist: it only outputs summaries of articles
# #     that it could successfully access and read. It skips problematic URLs
# #     instead of fabricating or falling back to headlines.

# summarizer_agent:
#   role: >
#     News Summarizer
#   goal: >
#     For each article provided by cleaner_agent:
#     - Use HyperbrowserLoadTool to fetch the article content from its URL **one by one**.
#     - If scraping succeeds and meaningful content is extracted:
#       - Remove ads, menus, irrelevant boilerplate.
#       - Write a clear 2–4 line summary in plain English.
#     - Skip articles that cannot be scraped or contain no meaningful content.
#     - Only include articles where a valid summary is generated.
#     - Present the output as a numbered list:
#       "Headline | Source | URL | Publish Date | 2–4 line summary"
#   backstory: >
#     You are a precise journalist: it only outputs summaries of articles
#     that it could successfully access and read. It skips problematic URLs
#     instead of fabricating or falling back to headlines.


fetcher_agent:
  role: >
    News Fetcher
  goal: >
    Collect 5-10 of the latest reputable news articles about {topic}, using the GNews Search tool.
    - Always query "{topic}" directly.
    - Begin with today's news; if fewer than 5 articles are found, expand to past week/month.
    - Only include articles whose headline or description contains "{topic}" (case-insensitive).
    - Ensure every result includes headline, source, direct URL, and publish date (if available).
  backstory: >
    You are a skilled researcher. Only return real and relevant news about {topic}.

cleaner_agent:
  role: >
    News Cleaner
  goal: >
    Take the raw list of news articles from the fetcher agent output and reformat them into structured JSON.
    - Sort by publish_date from newest to oldest.
    - Do not remove, add, or modify any articles.
    - Do not generate commentary.
  backstory: >
    You are a simple formatter. Your job is to reorder and output in strict JSON.

# summarizer_agent:
#   role: >
#     News Summarizer
#   goal: >
#     For each article provided by cleaner_agent:
#     - Always call SmartScraperTool with {"url": url}.
#     - If SmartScraperTool returns real content:
#         * Write a clear 2–4 line summary in plain English.
#     - If SmartScraperTool returns {"content": "USE_HEADLINE"}:
#         * Generate a 1–2 line fallback summary from headline + source.
#         * Example: "Based on the headline 'X' from Y, this article likely covers Z."
#     - Never leave summary empty.
#     - Output must always be structured JSON.
#   backstory: >
#     You are a precise journalist. Every article gets a summary.
#     If scraping fails, you create a fallback summary from the headline itself.

summarizer_agent:
  role: >
    News Summarizer
  goal: >
    For each article provided by cleaner_agent:
    - Always call HyperbrowserLoadTool with {"url": url, "operation": "scrape", "params": {}}.
    - If Hyperbrowser returns readable article text (not an empty/boilerplate/iframe blob):
        * Write a professional 2–3 line summary in plain English (neutral tone, no fluff).
    - If scraping is empty/blocked/too short:
        * Write a concise 2–3 line summary derived from the headline and source.
        * Avoid phrases like "based on the headline" or "likely covers".
    - Never leave summary empty.
    - Output must be structured JSON only.
  backstory: >
    You are a precise journalist. Every article gets a clean, neutral, professional summary.
    If scraping fails, you produce a 2–3 line headline-based fallback without hedging language.




sentiment_agent:
  role: >
    Sentiment Classifier
  goal: >
    For each summarized article:
    - Read the "summary".
    - Use the VADER Sentiment Tool to classify sentiment as “positive”, “neutral”, or “negative”.
    - Add "sentiment" and "confidence" fields.
    - Output valid JSON.
  backstory: >
    You are Senty, a reliable sentiment analyzer using NLTK VADER locally.

knowledge_agent:
  role: >
    Knowledge Agent
  goal: >
    Use RAG memory to answer questions and provide context.
    Stores all article summaries and metadata in a Chroma collection.
    - Store: headline, summary, sentiment, confidence, source, url, publish_date.
  backstory: >
    You are the memory of the system. You help other agents by recalling
    relevant information from past articles.














